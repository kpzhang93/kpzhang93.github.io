
<!DOCTYPE html>
<html>

  <head>
<!--    <meta http-equiv="refresh" content="0;url=http://graduatestudents.ucmerced.edu/wlai24/" /> -->
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="kpzhang93.github.io : ">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="kpzhang">
    <link rel="stylesheet" href="./src/main.css">
    <link rel="stylesheet" href="./src/bootstrap.min.css">
    <link rel="stylesheet" href="./src/bootstrap-theme.min.css">
 
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
 

    <link rel="stylesheet" href="./src/bootstrap-navbar-custom.css">
    <link rel="stylesheet" href="./src/scrolling-nav.css">
    <link rel="stylesheet" href="./src/mfp.css">
	<link rel="stylesheet" href="./src/font-awesome-4.7.0/css/font-awesome.min.css">
	
    <link rel="shortcut icon" href="./src/ntu.jpg">
    
    <script async="" src="./src/analytics.js"></script><script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-45156688-2', 'auto');
      ga('send', 'pageview');
    </script>

    <title>Kaipeng Zhang</title>
  </head>

  <body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">
<script async defer src="https://buttons.github.io/buttons.js"></script>
    <!-- HEADER -->
      <div class="navbar navbar-custom navbar-fixed-top top-nav-collapse" role="navigation">
        <div class="container">

          <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toogle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand page-scroll" href="https://kpzhang93.github.io/#page-top">Home</a>
          </div>

          <div class="navbar-collapse collapse">
            <ul class="nav navbar-nav">
              <li class="hidden"><a class="page-scroll" href="https://kpzhang93.github.io/#page-top"></a></li>
              <li class="active"><a class="page-scroll" href="https://kpzhang93.github.io/#about">About</a></li>
              <li><a class="page-scroll" href="https://kpzhang93.github.io/#publication">Publications</a></li>
              <li><a class="page-scroll" href="https://kpzhang93.github.io/#education">Education</a></li>
              <li><a class="page-scroll" href="https://kpzhang93.github.io/#academic">Academic Service</a></li>
              <li><a class="page-scroll" href="https://kpzhang93.github.io/#awards">Award</a></li>
			  <li><a class="page-scroll" href="https://kpzhang93.github.io/#work_experience">Work Experience</a></li>
	
            </ul>
          </div>

        </div>

      </div>

    <!-- MAIN CONTENT -->
    
    <section id="home" data-speed="0" data-type="background" class="home-section">
    
      <div class="container profile">
        <div class="row">
        
          <div class="col-md-4 col-xs-12 profile-img">
            <img src="./src/kpzhang.jpg" class="img-respective center-block" alt="Profile image" width="322" ,height="322">
          </div>
        

          <div class="col-md-6 col-xs-12 profile-info">
            <div><span class="english-name">Kaipeng Zhang</span>
            <div class="spacer"></div>
            <div class="info-list">
              <div>Researcher</div>
              <div>Shanghai AI Lab</div>
              <div class="email"><b>zhangkaipeng[at]pjlab.org.cn</b></div>
            </div>
            <div class="social-links">
              <a href="https://www.linkedin.com/in/kaipeng-zhang-0b750311b/" target="_blank"><i class="fa fa-linkedin-square fa-3x" aria-hidden="true"></i></a>
              <a href="https://github.com/kpzhang93" target="_blank"><i class="fa fa-github-square fa-3x" aria-hidden="true"></i></a>
              <a href="https://scholar.google.com/citations?user=4OqZBmYAAAAJ&hl=en" target="_blank"><i class="fa fa-graduation-cap fa-2x" aria-hidden="true"></i></a>
            </div>

          </div>
        </div>
        
      </div>
    
    </div></section>


    <section id="about" class="about-section">

      <div class="container">
        <div class="row">
          <div class="col-md-12 profile-description">
            <h2>About Me</h2>
            <hr>
            <div class="spacer"></div>
            <p>I am a researcher in Shanghai AI Lab and working on multimodal LLM and image/video generation.</p> 
            <p>Before coming to Shanghai AI Lab, I received my PhD degree in 2022 from The University of Tokyo, supervised by Prof. <a href='http://www.hci.iis.u-tokyo.ac.jp/~ysato/'>Yoichi Sato</a>, M.S. degree in 2018 from National Taiwan University supervised by Prof. <a href='https://winstonhsu.info/'>Winston Hsu</a>, and B.E. degree in 2016 from Donghua University. I was a researcher in SenseTime and consultant in ULSee. I also did research internships at Microsoft Research Asia, Tencent AI Lab, and MMLAB of SIAT. I currently work closely with Dr. <a href='http://mmlab.siat.ac.cn/yuqiao/'>Yu Qiao</a>, Dr. <a href='http://luoping.me/'>Ping Luo</a>, </a> and Dr. <a href='https://wqshao126.github.io/'>Wenqi Shao</a></p>
            <!--<p>My research interest mainly lies in computer vision, deep learning and focus on face analysis, active learning, and foundation vision models. I am fortunate to work closely with Dr. <a href='https://zhzhanp.github.io/'>Zhanpeng Zhang</a>, Dr. <a href='https://dblp.uni-trier.de/pers/hd/l/Li:Zhifeng'>Zhifeng Li</a>, Dr. <a href='https://scholar.google.com/citations?user=AjxoEpIAAAAJ&hl=zh-CN'>Wei Liu</a>, Dr. <a href='http://mmlab.siat.ac.cn/yuqiao/'>Yu Qiao</a> and <a href='http://ydwen.github.io'>Yandong Wen</a></p>!-->
            <p><span style="color:#FF0000;font-weight:bold;">I'm looking for interns interested in multimodal LLMs or image/video generation. Please send me your resume.</p>
            </p>
          </div>

        </div>
      </div> <!-- end of Description -->
    </section>
<!-- 
    <section id="news" class="news-section">

      <div class="container">
        <div class="row">
          <div class="col-md-12">
            <h2>News</h2>
            <hr>
            <div class="spacer"></div>
            <ul>
		<li><span class="glyphicon"></span> Dec., 2018: One IJCV paper has been accepted.</li> </li> 
		<li><span class="glyphicon"></span> Oct., 2018: One <a href='http://accv2018.net/'>ACCV</a> paper has been accepted as oral presentation.</li> </li> 
		<li><span class="glyphicon"></span> Sept., 2018: Start my full-time job at SenseTime.</li>
		<li><span class="glyphicon"></span> Jul., 2017: Our team won the second place in <a href='https://sites.google.com/site/emotiwchallenge/home'>EmotionW</a> (Group-based emotion recognition Challenge) at ICMI 2018 Grand Challenge.</li>
	    <li><span class="glyphicon"></span> Jul., 2018: One <a href='http://www.eccv2018.org/'>ECCV</a> paper has been accepted.</li> </li> 
	    <li><span class="glyphicon"></span> Jun., 2018: Our team won the first place in <a href='http://iab-rubric.org/DFW/dfw.html'>Disguised Faces in the Wild Challenge</a> at CVPR 2018.</li>
	    <li><span class="glyphicon"></span> Apr., 2018: Two <a href='http://cvpr2018.thecvf.com/'>CVPR workshop</a> papers have been accepted.</li> </li> 
	    
		<li><span class="glyphicon"></span> Jan., 2018: Start my internship at Microsoft Research Asia (MSRA). </li>
	    <li><span class="glyphicon"></span> Sept., 2017: Our team won the first place in <a href='https://sites.google.com/site/emotiwchallenge/home'>EmotionW</a> (Group-based emotion recognition Challenge) at ICMI 2017 Grand Challenge.</li>
	    
	    <li><span class="glyphicon"></span> July, 2017: One <a href='http://iccv2017.thecvf.com'>ICCV</a> paper has been accepted.</li>
		<li><span class="glyphicon"></span> July, 2017: Start my internship at Tencent AI Lab.</li>
            <li><span class="glyphicon"></span> August, 2016: One <a href='http://www.signalprocessingsociety.org/publications/periodicals/letters/'>SPL</a> paper for face detection has been accepted. Code&amp; Model is released. <iframe allowtransparency="true" scrolling="no" frameborder="0" src="https://buttons.github.io/buttons.html#href=https%3A%2F%2Fgithub.com%2Fkpzhang93%2FMTCNN_face_detection_alignment&amp;aria-label=Star%20kpzhang93%2FMTCNN_face_detection_alignment20on%20GitHub&amp;data-icon=octicon-star&amp;data-text=Star&amp;data-show-count=true" style="width: 90px; height: 20px; border: none;"></iframe>
			<iframe allowtransparency="true" scrolling="no" frameborder="0" src="https://buttons.github.io/buttons.html#href=https%3A%2F%2Fgithub.com%2Fkpzhang93%2FMTCNN_face_detection_alignment/fork&amp;aria-label=Fork%20kpzhang93%2FMTCNN_face_detection_alignment%20on%20GitHub&amp;data-icon=octicon-repo-forked&amp;data-text=Fork&amp;data-show-count=true" style="width: 90px; height: 20px; border: none;"></iframe></li>
            <li><span class="glyphicon"></span> July, 2016: One <a href='http://www.eccv2016.org'>ECCV</a> paper for face recognition has been accepted. Code&amp; Model is released. <iframe allowtransparency="true" scrolling="no" frameborder="0" src="https://buttons.github.io/buttons.html#href=https%3A%2F%2Fgithub.com%2Fydwen%2Fcaffe-face&amp;aria-label=Star%20ydwen%2Fcaffe-face%20on%20GitHub&amp;data-icon=octicon-star&amp;data-text=Star&amp;data-show-count=true" style="width: 90px; height: 20px; border: none;"></iframe>
			<iframe allowtransparency="true" scrolling="no" frameborder="0" src="https://buttons.github.io/buttons.html#href=https%3A%2F%2Fgithub.com%2Fydwen%2Fcaffe-face/fork&amp;aria-label=Fork%20ydwen%2Fcaffe-face%20on%20GitHub&amp;data-icon=octicon-repo-forked&amp;data-text=Fork&amp;data-show-count=true" style="width: 90px; height: 20px; border: none;"></iframe></li>
            <li><span class="glyphicon"></span> May, 2016: Our team won the first place in <a href='http://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge'>ChaLearn Looking at People Challenge</a> (tracks 2&amp; 3) at CVPR 2016.</li>
            <li><span class="glyphicon"></span> April, 2016: One <a href='http://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge'>CVPR workshop</a> paper has been accepted. </li>
            </ul>
            
          </div>

        </div>
      </div> 
    </section>
-->
	

    <section id="[publication" class="publication-section">
	
      <div class="container">
        <h2>Recent Publications (2024 & 2025)</h2>
        <hr>
		
		  <br>
			<div class="paper">[Recent Technical Report]</div>
			<div>
            <span class="label label-danger">[World Model]</span><a href="https://stdstu12.github.io/YUME-Project/" target="blank">Yume: An Interactive World Generation Model</a>
			</div> 
			<iframe width="560" height="315" src="https://www.youtube.com/embed/51VII_iJ1EM?si=3bCVvLg9BR0-WewE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
			</br>
			<div>
            <span class="label label-danger">[World Model]</span><a href="https://lixsp11.github.io/sekai-project/" target="blank">Sekai: A Video Dataset towards World Exploration</a>
			</div> 
			<iframe width="560" height="315" src="https://www.youtube.com/embed/5UQ0zAIZkSY?si=jeXoUoDebbmgtC9F" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
			</br>
			
			<div>
            <span class="label label-danger">[Agent]</span><a href="https://arxiv.org/abs/2507.07998" target="blank">PyVision: Agentic Vision with Dynamic Tooling</a>
			</div> 
			</br>

			<div>
            <span class="label label-danger">[Agent]</span><a href="https://arxiv.org/abs/2504.14191" target="blank">AI Idea Bench 2025: AI Research Idea Generation Benchmark</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Image Generation]</span><a href="https://arxiv.org/abs/2505.15779" target="blank">IA-T2I: Internet-Augmented Text-to-Image Generation</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Image Generation]</span><a href="https://arxiv.org/abs/2505.22126" target="blank">SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Image Generation]</span><a href="https://arxiv.org/abs/2505.16792" target="blank">REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Reasoning]</span><a href="https://arxiv.org/abs/2503.16188" target="blank">Think or not think: A study of explicit thinking in rule-based visual reinforcement fine-tuning</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Reasoning]</span><a href="https://arxiv.org/abs/2504.05782" target="blank">MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Reasoning]</span><a href="https://arxiv.org/abs/2505.11141" target="blank">Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Unified Multimodal Model]</span><a href="https://arxiv.org/abs/2506.09427" target="blank">A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation</a>
			</div> 
			</br>
			
			<div>
            <span class="label label-danger">[Unified Multimodal Model]</span><a href="https://arxiv.org/abs/2503.06542" target="blank">ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability</a>
			</div> 
			</br>
			
			<div class="paper">[Tutorial]</div>
			<div>
            <span class="label label-warning">[CVPR 2025 Tutorial]</span><a href="https://mllm2024.github.io/CVPR2025/" target="blank">From Multimodal LLM to Human-level AI: Evaluations and Benchmarks</a>
			</div> 
			</br>
			<div class="paper">[Conference Papers]</div>
			
			<div>
            <span class="label label-warning">[ICCV 2025]</span> <a href="" target="blank">ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges</a>
            <div class="author">Jiaxin Ai, Pengfei Zhou, xu Zhao Pan, Ming Li, Fanrui Zhang, Zizhen Li, Jianwen Sun, Yukang Feng, Baojin Huang, Zhongyuan Wang†, <b>Kaipeng Zhang†</b></div> 
			</div>
			
			<div>
            <span class="label label-warning">[ICCV 2025]</span> <a href="" target="blank">Neighboring Autoregressive Modeling for Efficient Visual Generation</a>
            <div class="author">Yefei He, Yuanyu He, Shaoxuan He, Feng Chen, Hong Zhou†, <b>Kaipeng Zhang†</b>, Bohan Zhuang†</b></div> 
			</div>
			
			<div>
            <span class="label label-warning">[ICCV 2025]</span> <a href="" target="blank">ZipVL: Accelerating Vision-Language Models through Dynamic Token Sparsity</a>
            <div class="author">Yefei He, Feng Chen, Jing Liu, Wenqi_Shao, Hong Zhou†, <b>Kaipeng Zhang†</b>, Bohan Zhuang </div> 
			</div>
			
			<div>
            <span class="label label-warning">[ICCV 2025]</span> <a href="" target="blank">LiT: Delving into a Simple Linear Diffusion Transformer for Image Generation</a>
            <div class="author">Jiahao Wang, Ning Kang, Lewei Yao, Mengzhao Chen, Chengyue Wu, Songyang Zhang, Shuchen Xue, Yong Liu, Taiqiang Wu, Xihui Liu, <b>Kaipeng Zhang</b>, Shifeng Zhang, Wenqi Shao†, Zhenguo Li†, Ping Luo</b></div> 
			</div>
			
			<div>
            <span class="label label-warning">[ICCV 2025]</span> <a href="" target="blank">	GUIOdyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices</a>
            <div class="author">Quanfeng_Lu, Wenqi Shao†, Zitao Liu, Lingxiao Du, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, <b>Kaipeng Zhang</b>, Ping Luo†</div> 
			</div>
			
			
			<div>
            <span class="label label-success">[ACL Findings 2025]</span> <a href="" target="blank">MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification</a>
            <div class="author">Zhaopan Xu, Pengfei Zhou, Jiaxin Ai, Wangbo Zhao, Kai Wang, Xiaojiang Peng, Wenqi Shao, Hongxun Yao†, <b>Kaipeng Zhang†</b></div> 
			</div>
			
			<div>
            <span class="label label-success">[ACL 2025]</span> <a href="" target="blank">	EfficientQAT: Efficient Quantization-Aware Training for Large Language Models</a>
            <div class="author">Mengzhao Chen, Wenqi Shao†, Peng Xu, Jiahao Wang, Peng Gao, <b>Kaipeng Zhang</b>, Ping Luo†</div> 
			</div>
			
			<div>
            <span class="label label-info">[ICML 2025]</span> <a href="https://arxiv.org/abs/2412.04062" target="blank">ZipAR: Accelerating Auto-regressive Image Generation through Spatial Locality</a>
            <div class="author">Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou†, <b>Kaipeng Zhang†</b>, Bohan Zhuang,  </div>
			</div>
			
			<div>
            <span class="label label-info">[ICML 2025]</span> <a href="https://arxiv.org/abs/2410.05363" target="blank">Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation</a>
            <div class="author">Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao†, Quanfeng Lu, <b>Kaipeng Zhang</b>, Yu Cheng, Dianqi Li, Ping Luo†</div>
			</div>
			
			<div>
            <span class="label label-danger">[IJCAI 2025]</span> <a href="https://arxiv.org/abs/2410.18071" target="blank">TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</a>
            <div class="author">Yuxuan Xie, Tianhua Li, Wenqi Shao, <b>Kaipeng Zhang†</b> </div>
			</div>

			<div>
            <span class="label label-warning">[CVPR 2025 Oral]</span> <a href="https://opening-benchmark.github.io/" target="blank">OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation</a>
            <div class="author">Pengfei Zhou*, Xiaopeng Peng*, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao,, Ping Luo, <b>Kaipeng Zhang†</b> </div>
			</div>

			<div>
            <span class="label label-default">[ICLR 2025 Oral]</span> <a href="https://openreview.net/pdf?id=X1OfiRYCLn" target="blank">Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping</a>
            <div class="author">Yue Yang*, Shuibo Zhang*, Wenqi Shao†, <b>Kaipeng Zhang†</b>, Yi Bin, Yu Wang, Ping Luo†</div> 
			</div>
			
			<div>
            <span class="label label-default">[ICLR 2025]</span> <a href="https://openreview.net/forum?id=JlDx2xp01W" target="blank">SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement</a>
            <div class="author">Yuqi Lin, Hengjia Li, Wenqi Shao, Zheng Yang†, Jun Zhao, Xiaofei He, Ping Luo, <b>Kaipeng Zhang†</b> </div>
			</div>
			
			<div>
            <span class="label label-default">[ICLR 2025]</span> <a href="https://openreview.net/pdf?id=WsgEWL8i0K" target="blank">MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models</a>
            <div class="author">Fanqing Meng*, Chuanhao Li*, Jin Wang*, Quanfeng Lu, Hao Tian, Tianshuo Yang, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, <b>Kaipeng Zhang†</b>, Wenqi Shao†</div> 
			</div>
		
			<div>
            <span class="label label-primary">[NeurIPS 2024 Spotlight]</span> <a href="https://arxiv.org/pdf/2403.20194" target="blank">ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models</a>
            <div class="author">Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao†, <b>Kaipeng Zhang†</b> </div>
			</div>
			
			<div>
            <span class="label label-primary">[NeurIPS 2024]</span> <a href="https://arxiv.org/abs/2405.14554" target="blank">SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge</a>
            <div class="author">Chuanhao Li, Zhen Li, Chenchen Jing, Shuo Liu, Wenqi Shao, Yuwei Wu, Ping Luo, Yu Qiao, <b>Kaipeng Zhang†</b> </div>
			</div>
			
			<div>
            <span class="label label-primary">[NeurIPS 2024]</span> <a href="https://arxiv.org/abs/2406.08845" target="blank">Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality</a>
            <div class="author">Tianle Zhang, Langtian Ma, Yuchen Yan, Yuchen Zhang, Kai Wang, Yue Yang, Ziyao Guo, Wenqi Shao, Yang You, Yu Qiao, Ping Luo, <b>Kaipeng Zhang†</b> </div>
			</div>
			
			<div>
            <span class="label label-primary">[NeurIPS 2024]</span> <a href="https://arxiv.org/abs/2406.07230" target="blank">Needle In A Multimodal Haystack</a>
            <div class="author">Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, <b>Kaipeng Zhang</b>, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao†, Wenhai Wang†</div>
			</div>
			
			<div>
            <span class="label label-primary">[NeurIPS 2024]</span> <a href="https://arxiv.org/abs/2406.18583" target="blank">Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT</a>
            <div class="author">Le Zhuo*, Ruoyi Du*, Han Xiao*, Yangguang Li*, Dongyang Liu*, Rongjie Huang*, Wenze Liu*, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, <b>Kaipeng Zhang</b>, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao†, Hongsheng Li†, Peng Gao†</div>
			</div>
		  
          
			<div>
            <span class="label label-info">[ICML 2024]</span> <a href="https://arxiv.org/abs/2403.02118" target="blank">Towards Implicit Prompt For Text-To-Image Models</a>
            <div class="author">Yue Yang, Yuqi Lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang, Yu Wang, Yu Qiao, <b>Kaipeng Zhang†</b> and Ping Luo†</div>
			</div>
			
			<div>
            <span class="label label-info">[ICML 2024]</span> <a href="https://arxiv.org/pdf/2404.16006" target="blank">MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI</a>
            <div class="author"> Kaining Ying*, Fanqing Meng*, Jin Wang*, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Cunjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, <b>Kaipeng Zhang†</b> and Wenqi Shao†</div>
			</div>
			
			<div>
            <span class="label label-info">[ICML 2024]</span> <a href="https://arxiv.org/abs/2402.05935" target="blank">SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</a>
            <div class="author">Peng Gao*†, Renrui Zhang*, Chris Liu*, Longtian Qiu*, Siyuan Huang*, Weifeng Lin*, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, <b>Kaipeng Zhang</b>, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li† and Yu Qiao</div>
			</div>
		
			<div>
            <span class="label label-warning">[CVPR 2024]</span> <a href="https://arxiv.org/pdf/2404.01342" target="blank">DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model</a>
            <div class="author">Lirui Zhao*, Yue Yang*, <b>Kaipeng Zhang‡*</b>, Wenqi Shao‡*, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji†</div> 
			</div>
			
			<div>
            <span class="label label-warning">[CVPR 2024]</span> <a href="https://arxiv.org/pdf/2312.03700" target="blank">OneLLM: One Framework to Align All Modalities with Language</a>
            <div class="author">Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, <b>Kaipeng Zhang</b>, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue†</div> 
			</div>
	
			<div>
            <span class="label label-success">[NAACL Findings 2024]</span> <a href="https://arxiv.org/abs/2408.12885" target="blank">T3M: Text Guided 3D Human Motion Synthesis from Speech</a>
            <div class="author">Wenshuo Peng, <b>Kaipeng Zhang†</b>, Sai Qian Zhang†</div> 
			</div>
			
			<div>
            <span class="label label-success">[ACL Findings 2024]</span> <a href="https://arxiv.org/pdf/2401.02384" target="blank">ChartAssistant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</a>
            <div class="author">Fanqing Meng, Wenqi Shao†, Quanfeng Lu, Peng Gao, <b>Kaipeng Zhang</b>, Yu Qiao, Ping Luo†</div> 
			</div>
			
			<div>
            <span class="label label-default">[ICLR 2024] </span><a href="https://arxiv.org/pdf/2310.05773" target="blank">Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching</a>
			<div class="author">Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, <b>Kaipeng Zhang†</b>, Yang You†</div> 
			</div>
      
	  
			<div>
            <span class="label label-default">[ICLR 2024] </span><a href="https://openreview.net/pdf?id=8Wuvhh0LYW" target="blank">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</a>
			<div class="author">Wenqi Shao*, Mengzhao Chen*, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, <b>Kaipeng Zhang</b>, Peng Gao, Yu Qiao, Ping Luo†</div> 
			</div>
			
			
			<div>
            <span class="label label-danger">[AAAI 2024]</span><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28249/28493" target="blank">Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification</a>
			<div class="author">Wenshuo Peng, <b>Kaipeng Zhang†</b>, Yue Yang, Hao Zhang, Yu Qiao</div> 
			</div>
			
			<div>
            <span class="label label-danger">[AAAI 2024]</span><a href="https://ojs.aaai.org/index.php/AAAI/article/view/28139/28281" target="blank">TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training</a>
			<div class="author">Yuqi Lin, Minghao Chen†, <b>Kaipeng Zhang†</b>, Hengjia Li, Mingming Li, Zheng Yang, Dongqin Lv, Binbin Lin, Haifeng Liu, Deng Cai</div> 
			</div>
			
			<div>
            <span class="label label-success">[ICASSP 2024]</span><a href="https://arxiv.org/pdf/2306.11504" target="blank">Align, Adapt and Inject: Audio-Guided Image Generation, Editing and Stylization</a>
			<div class="author">Yue Yang, <b>Kaipeng Zhang†</b>, Yuying Ge, Wenqi Shao, Zeyue Xue, Yu Qiao, Ping Luo†</div> 
			</div>

			</br>
			
			<div class="paper">[Journal Papers]</div>
			
			
			<div>
            <span class="label label-success">[IJCV 2024]</span><a href="https://arxiv.org/pdf/2310.05056" target="blank">Open-Vocabulary Animal Keypoint Detection with Semantic-feature Matching</a>
			<div class="author">Hao Zhang, Lumin Xu, Shenqi Lai, Wenqi Shao, Nanning Zheng†, Ping Luo, Yu Qiao, <b>Kaipeng Zhang†</b> </div>
			</div>
			
			
			<div>
            <span class="label label-success">[TPAMI 2024]</span><a href="https://arxiv.org/pdf/2306.09265" target="blank">LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</a>
			<div class="author">Peng Xu*, Wenqi Shao†*, <b>Kaipeng Zhang*</b>, Peng Gao*, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo†</div>
			</div>
			
			<div>
            <span class="label label-success">[TIFS 2024]</span><a href="https://arxiv.org/pdf/2403.09346" target="blank">B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions</a>
			<div class="author">Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Nanning Zheng†, <b>Kaipeng Zhang†</b></div>
			</div>
			
			<div>
            <span class="label label-success">[TBigData 2024]</span><a href="https://arxiv.org/pdf/2308.03729" target="blank">Tiny LVLM-eHub: Early Multimodal Experiments with Bard</a>
			<div class="author">Wenqi Shao*, Yutao Hu*, Peng Gao*, Meng Lei*, <b>Kaipeng Zhang</b>, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao†, Ping Luo†</div>
			</div>
			
			<div>
            <span class="label label-success">[IJCV 2024]</span><a href="https://arxiv.org/pdf/2310.05056" target="blank">Open-Vocabulary Animal Keypoint Detection with Semantic-feature Matching</a>
			<div class="author">Hao Zhang, Lumin Xu, Shenqi Lai, Wenqi Shao, Nanning Zheng†, Ping Luo, Yu Qiao, <b>Kaipeng Zhang†</b> </div>
			</div>
		  
			<div>
            <span class="label label-success">[TCSVT 2024]</span><a href="https://ieeexplore.ieee.org/document/10472506/" target="blank">HF-HRNet: a simple hardware friendly high-resolution network</a>
			<div class="author">Hao Zhang, Yujie Dun, Yixuan Pei, Shenqi Lai, Chengxu Liu, <b>Kaipeng Zhang</b>, Xueming Qian†</div>
			</div>
	
			<div>
            <span class="label label-success">[Pattern Recognition 2024]</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320324004497" target="blank">FMGNet: An efficient feature-multiplex group network for real-time vision task</a>
			<div class="author">Hao Zhang, Yongqiang Ma, <b>Kaipeng Zhang</b>, Nanning Zheng†, Shenqi Lai†</div>
			</div>
	</br>
    </section>

    <section id="education" class="education-section">
      <div class="container">
        <h2>Education</h2>
        <hr>
        <div class="spacer"></div>
		
		<div class="row item">
          <div class="col-md-4 col-sm-6 col-xs-12">
            <a href="https://www.i.u-tokyo.ac.jp/index_e.shtml" target="_blank">
              <img src="./src/utokyo.png" style="width:140px;height:140px;" align="center" class="img-responsive edu-img">
            </a>
          </div>
          <div class="col-md-8 col-sm-6 col-xs-12 item-info">
            
            <div class="row edu">
              <div class="col-md-12 col-xs-12 edu-info">
                <div class="degree"><b>Ph.d.</b> in CS, The University of Tokyo, Tokyo, Japan</div>
                <div class="date">Apr. 2019 - Mar. 2022</div>
              </div>
            </div>
    
          </div>
        </div>
		
        <div class="row item">
          <div class="col-md-4 col-sm-6 col-xs-12">
            <a href="http://www.ntu.edu.tw/" target="_blank">
              <img src="./src/ntu.jpg" style="width:140px;height:140px;" align="center" class="img-responsive edu-img">
            </a>
          </div>
          <div class="col-md-8 col-sm-6 col-xs-12 item-info">
            
            <div class="row edu">
              <div class="col-md-12 col-xs-12 edu-info">
                <div class="degree"><b>M.S.</b> in CS, National Taiwan University, Taipei, Taiwan</div>
                <div class="date">Sep. 2016 - Aug. 2018</div>
              </div>
            </div>
    
          </div>
        </div>
		<div class="row item">
          <div class="col-md-4 col-sm-6 col-xs-12">
            <a href="http://english.dhu.edu.cn/" target="_blank">
              <img src="./src/donghua.jpg" style="width:140px;height:140px;" align="center" class="img-responsive edu-img">
            </a>
          </div>
          <div class="col-md-8 col-sm-6 col-xs-12 item-info">
            
            <div class="row edu">
              <div class="col-md-12 col-xs-12 edu-info">
                <div class="degree"><b>B.Eng.</b> in CS, Donghua University, Shanghai, China</div>
                <div class="date">Sep. 2012 - July 2016</div>
              </div>
            </div>
    
          </div>
        </div>
      </div> <!-- end of Education -->
	  
	  <section id="awards" class="awards-section">
      <div class="container">
        <h2>Selected Awards and Competitions</h2>
        <hr>
        <div class="spacer"></div>
		 <li><span class="glyphicon"></span><a href='https://www.worldaic.com.cn/activity#q0'>WAIC Young Outstanding Paper Award</a>, 2022</li>
         <li><span class="glyphicon"></span>World's TOP 2% Scientists (published by Stanford University), 2020 & 2021 & 2022 & 2023</li>
         <li><span class="glyphicon"></span>JSPS Research Fellowships for Young Scientists, 2020</li>
		 
         <li><span class="glyphicon"></span>Tencent Rhino-Bird Elite Training Program, 2020</li>
         <li><span class="glyphicon"></span>MSRA Fellowship Nomination Award, 2019</li>
         <li><span class="glyphicon"></span><a href='https://sites.google.com/view/emotiw2019'>Emotion Recognition in the Wild</a>: Engagement Prediction (ICMI 2019 Grand Challenge), 3rd place</li>
         <li><span class="glyphicon"></span><a href='https://sites.google.com/view/emotiw2019'>Emotion Recognition in the Wild</a>: Group-based Cohesion Prediction (ICMI 2019 Grand Challenge), 2nd place</li>
         <li><span class="glyphicon"></span><a href='http://iab-rubric.org/DFW/dfw.html'>Disguised Faces in the Wild Challenge</a> (in conjunction with CVPR 2018), 1st place</li>
         <li><span class="glyphicon"></span><a href='https://sites.google.com/view/emotiw2018'>Emotion Recognition in the Wild</a>: Group-level emotion recognition (ICMI 2018 Grand Challenge), 2nd place</li>
		 <li><span class="glyphicon"></span><a href='https://sites.google.com/site/emotiwchallenge/home'>Emotion Recognition in the Wild</a>: Group-level emotion recognition (ICMI 2017 Grand Challenge), 1st place</li>
         
         <li><span class="glyphicon"></span><a href='http://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge'>ChaLearn Looking at People Challenge</a>: Accessories Classification (in conjunction with CVPR 2016), 1st place</li>
         <li><span class="glyphicon"></span><a href='http://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge'>ChaLearn Looking at People Challenge</a>: Smile and Gender Classification (in conjunction with CVPR 2016), 1st place</li>
         <li><span class="glyphicon"></span>Outstanding Undergraduate Thesis, 2016</li>
         </div>
    </section>
	
	<section id="academic" class="academic-section">
      <div class="container"> 
        <h2>Academic Service</h2>
        <hr>
        <div class="spacer"></div>
			
			<li><span class="glyphicon"></span>Senior program committee of IJCAI</li>
			<li><span class="glyphicon"></span>Reviewer/Program committee of NeurIPS, ICML, ICLR, AAAI, ICCV, ECCV, CVPR, BMVC, WACV and ACCV</li>
			<li><span class="glyphicon"></span>Reviewer of TPAMI, TIP, TCSVT, TNNLS, TMM, TIFS, Neurocomputing, Pattern Recognition, and SPL</li>
      
         </div>
    </section>
	
    
    <section id="work_experience" class="work_experience_section">    
        <div class="container">
            <h2>Work Experience</h2>
            <hr>
			
                <div class="spacer"></div>
				<div class="row item">
            <div><a href="https://www.shlab.org.cn/" target="_blank">
                    <img src="./src/shailab.png" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                </a></div>
                <div class="col-md-5" align="left">
                <div><b>Researcher</b></div>
                <div>Shanghai AI Lab</div>
				<div>OpenGVLab</div>
				<div>Shanghai, China</div>
                <div class="date">May. 2022 - Present</div>
            </div>
            <div class="row item">
            <div><a href="https://www.sensetime.com/" target="_blank">
                    <img src="./src/sensetime.png" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                </a></div>
                <div class="col-md-5" align="left">
                <div><b>Researcher</b></div>
                <div>SenseTime</div>
				<div>Research Institute</div>
				<div>Shenzhen, China</div>
                <div class="date">Sept. 2018 - Mar. 2019</div>
            </div>
			</div>
			
			
			<div class="row item">
			<div><a href="https://www.msra.cn/" target="_blank">
                    <img src="./src/ms.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                </a></div>
                <div class="col-md-5" align="left">
                <div><b>Intern</b></div>
                <div>MSRA</div>
                <div>Visual Computing Group</div>
				<div>Beijing, China</div>
                <div class="date">Jan. 2018 - Jul. 2018</div>
            </div>
          <div class="row item">
			<div><a href="https://ulsee.com/" target="_blank">
                    <img src="./src/ulsee.png" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                </a></div>
                <div class="col-md-5" align="left">
                <div><b>Consultant</b></div>
                <div>ULSee</div>
                <div>Face Team</div>
				<div>Hangzhou, China</div>
                <div class="date">Oct. 2016 - Mar. 2018</div>
				
            </div>
            </div>
			
			
			
			<div><a href="http://ai.tencent.com/" target="_blank">
                    <img src="./src/tencent.png" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                </a></div>
                <div class="col-md-5" align="left">
                <div><b>Intern</b></div>
                <div>Tencen</div>
                <div>AI Lab & AI Advertisement Department</div>
				<div>Shenzhen, China</div>
                <div class="date">Jul. 2017 - Aug. 2017</div>
                <div class="date">Sep. 2020 - Feb. 2021</div>
				
            </div>
		    
            <div class="row item">
            <div><a href="http://mmlab.siat.ac.cn/" target="_blank">
                    <img src="./src/siat.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
                    <div class="col-md-5" align="left">
                    <div><b>Visiting Student</b></div>
                    <div>Shenzhen Institutes of Advanced Technology</div>
                    <div>Multimedia Research Center</div>
					<div>Shenzhen, China</div>
                    <div class="date">Jul. 2015 - Aug. 2016</div>
            </div>
            </div>
    </div> <!-- end of Experience -->
	</section>

	 <!--
	
	<section id="interest" class="interest-section">
      <div class="container">
        <h2>Interest</h2>
        <hr>
        <div class="spacer"></div>
			<div class="row item">
			<div><a href="https://weibo.com/xuanyi0808?topnav=1&wvr=6&topsug=1&is_hot=1" target="_blank">						
              <img src="./src/xy.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
			
			
                <div><a href="https://weibo.com/meiqi1015?topnav=1&wvr=6&topsug=1" target="_blank">
                    <img src="./src/shanzhi.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				
				<div><a href="http://weibo.com/u/5225534295?is_all=1/" target="_blank">						
              <img src="./src/xx.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
                    <div><a href="http://weibo.com/u/3675601605?is_all=1/" target="_blank">						
              <img src="./src/jiaai.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
										
				<div><a href="http://weibo.com/u/3669120105?topnav=1&wvr=6&topsug=1&is_all=1/" target="_blank">						
              <img src="./src/luting.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="http://weibo.com/u/3675868752?topnav=1&wvr=6&topsug=1" target="_blank">						
              <img src="./src/nanaxi.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="http://weibo.com/u/3705941004?refer_flag=0000015012_&from=feed&loc=avatar&is_all=1" target="_blank">						
              <img src="./src/nana.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="https://weibo.com/u/5690216533?topnav=1&wvr=6&topsug=1" target="_blank">						
              <img src="./src/zhangyi.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
            </div>
			<div class="row item">
                <div><a href="https://en.wikipedia.org/wiki/Code_Geass" target="_blank">
                    <img src="./src/codegeass.png" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="https://en.wikipedia.org/wiki/Sword_Art_Online" target="_blank">						
              <img src="./src/sao.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="https://en.wikipedia.org/wiki/Log_Horizon" target="_blank">						
              <img src="./src/log.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
                    <div><a href="https://en.wikipedia.org/wiki/K-On!" target="_blank">						
              <img src="./src/kon.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
										
				<div><a href="https://en.wikipedia.org/wiki/One-Punch_Man" target="_blank">						
              <img src="./src/onep.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="https://en.wikipedia.org/wiki/Attack_on_Titan" target="_blank">						
              <img src="./src/titan.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="https://en.wikipedia.org/wiki/Fullmetal_Alchemist" target="_blank">						
              <img src="./src/fa.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
				<div><a href="https://en.wikipedia.org/wiki/Fate/stay_night" target="_blank">						
              <img src="./src/fate.jpg" style="width:140px;height:140px;" align="left" class="img-responsive edu-img">
                                        </a></div>
            </div>
         </div>
         </div>
    </section>
	
		<section id="friend" class="friend-section">
      <div class="container">
        <h2>Friends</h2>
        <hr>
        <div class="spacer"></div>
         <a href="https://zhzhanp.github.io/">Zhanpeng Zhang</a> (SenseTime),
<a href="http://ydwen.github.io/">Yandong Wen</a> (Max Planck Institute),
<a href="http://wangzheallen.github.io/">Zhe Wang</a> (Adobe),
<a href="http://tonghe90.github.io/">Tong He</a> (Shanghai AI Lab),
<a href="https://zhitian.xyz/">Zhi Tian</a> (ByteDance),
<a href="http://guoshengcv.github.io/">Sheng Guo</a> (Ant),
<a href="http://zbwglory.github.io/">Bowen Zhang</a> (Apple),
<a href="http://bestsonny.github.io/">Pan He</a> (Auburn University),
<a href="https://www.linkedin.com/in/jiaming-liu/">Jiaming Liu</a> (Microsoft),
<a href="https://www.linkedin.com/in/cheng-han-lee-109a45127/">Cheng-Han Li</a> (UT Austin),
<a href="http://www.cmlab.csie.ntu.edu.tw/~chiawen/">Chia-Wen Cheng</a> (Microsoft),
<a href="https://hyf015.github.io/">Yifei Huang</a> (Shanghai AI Lab),
<a href="http://lightchaserx.github.io/">Zhixiang Wang</a> (UTokyo),
<a href="https://wangzwhu.github.io/home/">Zheng Wang</a> (Wuhan University),
<a href="https://www.linkedin.com/in/lixiong-chen-53232a17/">Lixiong Chen</a> (Oxford)
         </div>
    </section>
	
</div></section> -->
    <div class="footer">
      <div class="container">
        <div class="row item">
		
        <p>Copyright © Kaipeng Zhang 2017</p>
        <p>Template from <a href="http://phoenix104104.github.io">Jason Lai</a></p>
        <!--
        <p class="last-update">
          Last Updated: 
          <script language="JavaScript">
            document.write(document.lastModified);
          </script>
        </p>
		 <div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3202466&c=9607711" alt="AmazingCounters.com"></a></div> -->
      </div>
	  
    </div>
	
    <script type="text/javascript" src="./src/jquery-1.11.0.js"></script>
    <script src="./src/bootstrap.min.js"></script>
    <!--
    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script src="//code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    -->
    <!-- Scrolling Nav JavaScript -->
    <script src="./src/jquery.easing.min.js"></script>
    <script src="./src/scrolling-nav.js"></script>
    <script src="./src/mfp.js"></script>
    <script type="text/javascript" src="./src/main.js"></script>
    
  

</div></body></html>